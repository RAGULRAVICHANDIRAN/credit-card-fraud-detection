{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“– Notebook 02 â€” Existing Work Replication\n",
                "\n",
                "Replicates the paper's methodology:\n",
                "- **Models**: Naive Bayes, Random Forest (n=5), XGBoost (n=5)\n",
                "- **Strategies**: Original, Oversampling, Undersampling, SMOTE\n",
                "- **Datasets**: European & Sparkov\n",
                "- **Split**: 70 / 15 / 15  |  **CV**: 5-fold\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os, warnings\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import cross_val_score, KFold\n",
                "import joblib\n",
                "\n",
                "from src.utils.config import (\n",
                "    RANDOM_SEED, MODELS_DIR, N_SPLITS,\n",
                "    DS_EUROPEAN, DS_SPARKOV,\n",
                "    MODEL_NB, MODEL_RF, MODEL_XGB,\n",
                "    ALL_STRATEGIES,\n",
                ")\n",
                "from src.utils.metrics import evaluate_model, results_to_dataframe, get_roc_curve\n",
                "from src.data.preprocessing import preprocess_european, preprocess_sparkov, load_processed\n",
                "from src.data.balancing_strategies import get_balanced_datasets, describe_balance\n",
                "from src.models.baseline_models import get_naive_bayes\n",
                "from src.models.ensemble_models import get_random_forest, get_xgboost\n",
                "from src.visualization.plot_utils import (\n",
                "    plot_roc_curves, plot_performance_by_dataset,\n",
                "    plot_performance_per_dataset, plot_f1_density,\n",
                "    plot_average_performance, plot_confusion_matrix,\n",
                "    plot_methodology_flowchart,\n",
                ")\n",
                "\n",
                "np.random.seed(RANDOM_SEED)\n",
                "%matplotlib inline\n",
                "print('Setup complete.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load & Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess (run once, then use load_processed)\n",
                "# eu_data = preprocess_european()\n",
                "# sp_data = preprocess_sparkov()\n",
                "\n",
                "eu_data = load_processed(DS_EUROPEAN)\n",
                "sp_data = load_processed(DS_SPARKOV)\n",
                "\n",
                "datasets = {\n",
                "    DS_EUROPEAN: eu_data,\n",
                "    DS_SPARKOV:  sp_data,\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Balanced Variants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "balanced_sets = {}\n",
                "for ds_name, data in datasets.items():\n",
                "    balanced_sets[ds_name] = get_balanced_datasets(data['X_train'], data['y_train'])\n",
                "    for strat, (X, y) in balanced_sets[ds_name].items():\n",
                "        describe_balance(y, f'{ds_name}/{strat}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train All Existing Models (3 models Ã— 4 strategies Ã— 2 datasets = 24 runs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_factories = {\n",
                "    MODEL_NB:  get_naive_bayes,\n",
                "    MODEL_RF:  lambda: get_random_forest(paper_params=True),\n",
                "    MODEL_XGB: lambda: get_xgboost(paper_params=True),\n",
                "}\n",
                "\n",
                "all_results = {}       # {ds: {strat: {model: metrics}}}\n",
                "trained_models = {}    # {(ds, strat, model): fitted_model}\n",
                "roc_data_collection = []  # for ROC plots\n",
                "\n",
                "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
                "\n",
                "for ds_name, data in datasets.items():\n",
                "    all_results[ds_name] = {}\n",
                "    X_test, y_test = data['X_test'], data['y_test']\n",
                "\n",
                "    for strat, (X_bal, y_bal) in balanced_sets[ds_name].items():\n",
                "        all_results[ds_name][strat] = {}\n",
                "        print(f'\\n=== {ds_name} / {strat} ===')\n",
                "\n",
                "        for model_name, factory in model_factories.items():\n",
                "            model = factory()\n",
                "\n",
                "            # Cross-validation scores\n",
                "            cv_f1 = cross_val_score(model, X_bal, y_bal, cv=kf, scoring='f1')\n",
                "\n",
                "            # Full train & evaluate\n",
                "            model.fit(X_bal, y_bal)\n",
                "            y_pred = model.predict(X_test)\n",
                "            y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
                "            metrics = evaluate_model(y_test, y_pred, y_prob)\n",
                "            metrics['cv_f1_mean'] = cv_f1.mean()\n",
                "            metrics['cv_f1_std'] = cv_f1.std()\n",
                "\n",
                "            all_results[ds_name][strat][model_name] = metrics\n",
                "            trained_models[(ds_name, strat, model_name)] = model\n",
                "\n",
                "            # Collect ROC data\n",
                "            if y_prob is not None:\n",
                "                roc_data_collection.append({\n",
                "                    'label': f'{model_name} / {ds_name} / {strat}',\n",
                "                    'y_true': y_test, 'y_prob': y_prob,\n",
                "                })\n",
                "\n",
                "            print(f'  {model_name:15s}  F1={metrics[\"f1\"]:.4f}  '\n",
                "                  f'AUC={metrics[\"roc_auc\"]:.4f}  '\n",
                "                  f'CV-F1={cv_f1.mean():.4f}Â±{cv_f1.std():.4f}')\n",
                "\n",
                "            # Save model\n",
                "            joblib.dump(model, MODELS_DIR / f'{ds_name}_{strat}_{model_name}.joblib')\n",
                "\n",
                "print('\\nâœ“ All existing models trained and saved.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Results Summary Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = results_to_dataframe(all_results)\n",
                "results_df.to_csv(MODELS_DIR / 'existing_results.csv', index=False)\n",
                "\n",
                "styled = results_df.style.format({\n",
                "    'accuracy': '{:.4f}', 'precision': '{:.4f}',\n",
                "    'recall': '{:.4f}', 'f1': '{:.4f}', 'roc_auc': '{:.4f}'\n",
                "}).background_gradient(subset=['f1', 'roc_auc'], cmap='YlGn')\n",
                "\n",
                "styled"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualisations (Paper Figures)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 1: Methodology Flowchart\n",
                "plot_methodology_flowchart()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 2: Sample ROC Curves\n",
                "sample_roc = [\n",
                "    r for r in roc_data_collection\n",
                "    if ('naive_bayes' in r['label'] and 'european' in r['label'] and 'oversampled' in r['label'])\n",
                "    or ('random_forest' in r['label'] and 'european' in r['label'] and 'smote' in r['label'])\n",
                "    or ('xgboost' in r['label'] and 'sparkov' in r['label'] and 'undersampled' in r['label'])\n",
                "]\n",
                "if sample_roc:\n",
                "    plot_roc_curves(sample_roc, title='ROC Curves â€“ Sample Models')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 3: Performance by Dataset\n",
                "plot_performance_by_dataset(results_df)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 4 & 5: Performance on each dataset\n",
                "plot_performance_per_dataset(results_df, DS_EUROPEAN)\n",
                "plt.show()\n",
                "\n",
                "plot_performance_per_dataset(results_df, DS_SPARKOV)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 6: F1 Density Distributions\n",
                "plot_f1_density(results_df)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 7: Average Performance & Gap\n",
                "plot_average_performance(results_df)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Analysis of Existing Work Results\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **Ensemble methods outperform Naive Bayes**: Both RF and XGB achieve higher F1 and AUC scores across all dataset Ã— strategy combinations.\n",
                "2. **SMOTE yields the best overall performance**: Synthetic minority oversampling provides the most balanced training signal, especially for recall.\n",
                "3. **Real data (EU) > Simulated data (Sparkov)**: The deterministic approval scripts in real-world systems create learnable fraud patterns, while the simulated data has higher stochasticity.\n",
                "4. **Undersampling degrades performance**: Losing majority-class samples reduces the model's ability to distinguish legitimate transactions.\n",
                "\n",
                "---\n",
                "*Proceed to Notebook 03 for proposed deep-learning models.*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}