{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Notebook 03 â€” Proposed Model Development\n",
                "\n",
                "Implements the proposed enhancements beyond the original paper:\n",
                "1. **CNN-BiGRU** hybrid deep-learning model\n",
                "2. **BERT-based** classifier (DistilBERT on tabularâ†’text)\n",
                "3. **Stacking Ensemble** (RF + XGB + CNN-BiGRU â†’ Logistic Regression)\n",
                "4. **Hyperparameter Tuning** for RF & XGB\n",
                "5. **Explainability** (SHAP, LIME, Feature Importance)\n",
                "6. **2FA / MFA Simulation** framework\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os, warnings\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import joblib\n",
                "\n",
                "from src.utils.config import (\n",
                "    RANDOM_SEED, MODELS_DIR, FIGURES_DIR, N_SPLITS,\n",
                "    DS_EUROPEAN, DS_SPARKOV,\n",
                ")\n",
                "from src.utils.metrics import evaluate_model, results_to_dataframe\n",
                "from src.data.preprocessing import load_processed\n",
                "from src.data.balancing_strategies import get_balanced_datasets\n",
                "\n",
                "np.random.seed(RANDOM_SEED)\n",
                "%matplotlib inline\n",
                "print('Setup complete.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data (use European dataset as primary)\n",
                "eu_data = load_processed(DS_EUROPEAN)\n",
                "balanced = get_balanced_datasets(eu_data['X_train'], eu_data['y_train'])\n",
                "X_smote, y_smote = balanced['smote']\n",
                "X_val, y_val = eu_data['X_val'], eu_data['y_val']\n",
                "X_test, y_test = eu_data['X_test'], eu_data['y_test']\n",
                "print(f'SMOTE training: {len(X_smote):,} samples')\n",
                "print(f'Validation:     {len(X_val):,} samples')\n",
                "print(f'Test:           {len(X_test):,} samples')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. CNN-BiGRU Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.deep_learning_models import (\n",
                "    build_cnn_bigru, compile_cnn_bigru, train_cnn_bigru, reshape_for_cnn\n",
                ")\n",
                "from src.visualization.plot_utils import plot_training_history\n",
                "\n",
                "# Reshape for Conv1D\n",
                "X_tr_3d = reshape_for_cnn(X_smote)\n",
                "X_val_3d = reshape_for_cnn(X_val)\n",
                "X_te_3d = reshape_for_cnn(X_test)\n",
                "\n",
                "print(f'Input shape: {X_tr_3d.shape[1:]}')\n",
                "\n",
                "cnn_model = build_cnn_bigru(input_shape=(X_tr_3d.shape[1], 1))\n",
                "cnn_model = compile_cnn_bigru(cnn_model)\n",
                "cnn_model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = train_cnn_bigru(cnn_model, X_tr_3d, y_smote, X_val_3d, y_val)\n",
                "plot_training_history(history, model_name='CNN-BiGRU')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "y_prob_cnn = cnn_model.predict(X_te_3d).flatten()\n",
                "y_pred_cnn = (y_prob_cnn >= 0.5).astype(int)\n",
                "cnn_metrics = evaluate_model(y_test, y_pred_cnn, y_prob_cnn)\n",
                "print(f\"CNN-BiGRU â€” F1: {cnn_metrics['f1']:.4f}  AUC: {cnn_metrics['roc_auc']:.4f}\")\n",
                "\n",
                "cnn_model.save(str(MODELS_DIR / 'european_smote_cnn_bigru.h5'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. BERT-based Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.deep_learning_models import BertFraudClassifier\n",
                "\n",
                "# Sub-sample for feasibility\n",
                "N_BERT = 3000\n",
                "idx = np.random.choice(len(X_smote), N_BERT, replace=False)\n",
                "X_bert_tr = X_smote.iloc[idx] if hasattr(X_smote, 'iloc') else X_smote[idx]\n",
                "y_bert_tr = y_smote.iloc[idx] if hasattr(y_smote, 'iloc') else y_smote[idx]\n",
                "\n",
                "X_bert_te = X_test.iloc[:1000] if hasattr(X_test, 'iloc') else X_test[:1000]\n",
                "y_bert_te = y_test.iloc[:1000] if hasattr(y_test, 'iloc') else y_test[:1000]\n",
                "\n",
                "bert_clf = BertFraudClassifier()\n",
                "bert_clf.prepare_data(X_bert_tr, y_bert_tr)\n",
                "bert_clf.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_prob_bert = bert_clf.predict_proba(X_bert_te)\n",
                "y_pred_bert = (y_prob_bert >= 0.5).astype(int)\n",
                "bert_metrics = evaluate_model(y_bert_te, y_pred_bert, y_prob_bert)\n",
                "print(f\"BERT â€” F1: {bert_metrics['f1']:.4f}  AUC: {bert_metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stacking Ensemble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.ensemble_models import get_stacking_ensemble\n",
                "\n",
                "stacking = get_stacking_ensemble()\n",
                "stacking.fit(X_smote, y_smote)\n",
                "\n",
                "y_pred_stack = stacking.predict(X_test)\n",
                "y_prob_stack = stacking.predict_proba(X_test)[:, 1]\n",
                "stack_metrics = evaluate_model(y_test, y_pred_stack, y_prob_stack)\n",
                "print(f\"Stacking â€” F1: {stack_metrics['f1']:.4f}  AUC: {stack_metrics['roc_auc']:.4f}\")\n",
                "\n",
                "joblib.dump(stacking, MODELS_DIR / 'european_smote_stacking.joblib')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.ensemble_models import tune_random_forest, tune_xgboost\n",
                "\n",
                "best_rf, rf_search = tune_random_forest(X_smote, y_smote, n_iter=30)\n",
                "joblib.dump(best_rf, MODELS_DIR / 'tuned_rf.joblib')\n",
                "\n",
                "y_pred_trf = best_rf.predict(X_test)\n",
                "y_prob_trf = best_rf.predict_proba(X_test)[:, 1]\n",
                "trf_metrics = evaluate_model(y_test, y_pred_trf, y_prob_trf)\n",
                "print(f\"Tuned RF â€” F1: {trf_metrics['f1']:.4f}  AUC: {trf_metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_xgb, xgb_search = tune_xgboost(X_smote, y_smote, n_iter=30)\n",
                "joblib.dump(best_xgb, MODELS_DIR / 'tuned_xgb.joblib')\n",
                "\n",
                "y_pred_txgb = best_xgb.predict(X_test)\n",
                "y_prob_txgb = best_xgb.predict_proba(X_test)[:, 1]\n",
                "txgb_metrics = evaluate_model(y_test, y_pred_txgb, y_prob_txgb)\n",
                "print(f\"Tuned XGB â€” F1: {txgb_metrics['f1']:.4f}  AUC: {txgb_metrics['roc_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Explainability (SHAP & LIME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.explainability import shap_analysis, shap_force_plot, lime_analysis, feature_importance_report\n",
                "\n",
                "# SHAP on tuned XGB\n",
                "shap_vals = shap_analysis(best_xgb, X_test, model_type='tree')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP force plot for a fraudulent instance\n",
                "fraud_indices = y_test[y_test == 1].index.tolist()\n",
                "if fraud_indices:\n",
                "    # Find position in X_test\n",
                "    fraud_pos = X_test.index.get_loc(fraud_indices[0]) if hasattr(X_test, 'index') else 0\n",
                "    shap_force_plot(best_xgb, X_test, instance_idx=fraud_pos)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LIME explanation\n",
                "exp = lime_analysis(best_xgb, X_smote, X_test, instance_idx=0)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "fi_report = feature_importance_report(best_xgb, X_test, y_test)\n",
                "fi_report.head(15)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 2FA / MFA Simulation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_2fa(risk_scores, threshold_2fa=0.5, threshold_mfa=0.8):\n",
                "    \"\"\"\n",
                "    Multi-layer authentication simulation.\n",
                "    Layer 1: Standard auth (risk < threshold_2fa)\n",
                "    Layer 2: 2FA â€” SMS/Email (threshold_2fa <= risk < threshold_mfa)\n",
                "    Layer 3: MFA â€” Biometric (risk >= threshold_mfa)\n",
                "    \"\"\"\n",
                "    layers = np.where(\n",
                "        risk_scores < threshold_2fa, 'Standard',\n",
                "        np.where(risk_scores < threshold_mfa, '2FA', 'MFA')\n",
                "    )\n",
                "    return layers\n",
                "\n",
                "# Use model predictions as risk scores\n",
                "risk = y_prob_txgb  # tuned XGB probabilities\n",
                "auth_layers = simulate_2fa(risk)\n",
                "\n",
                "sim_df = pd.DataFrame({\n",
                "    'risk_score': risk,\n",
                "    'true_label': y_test.values,\n",
                "    'auth_layer': auth_layers,\n",
                "})\n",
                "\n",
                "print('=== Authentication Layer Distribution ===')\n",
                "print(sim_df['auth_layer'].value_counts())\n",
                "\n",
                "print('\\n=== Fraud caught per layer ===')\n",
                "caught = sim_df[sim_df['true_label'] == 1].groupby('auth_layer').size()\n",
                "print(caught)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "sim_df['auth_layer'].value_counts().plot(kind='bar', ax=axes[0],\n",
                "    color=['#55A868', '#F0AD4E', '#C44E52'], edgecolor='black')\n",
                "axes[0].set_title('Transactions per Auth Layer')\n",
                "\n",
                "for layer, color in zip(['Standard', '2FA', 'MFA'],\n",
                "                        ['#55A868', '#F0AD4E', '#C44E52']):\n",
                "    subset = sim_df[sim_df['auth_layer'] == layer]['risk_score']\n",
                "    axes[1].hist(subset, bins=30, alpha=0.5, color=color, label=layer)\n",
                "axes[1].set_title('Risk Score Distribution by Auth Layer')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / '2fa_simulation.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Engineering (Proposed)\n",
                "\n",
                "Demonstrates new features that could improve detection when raw transaction data is available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def engineer_features(df):\n",
                "    \"\"\"Create new features from raw transaction data.\"\"\"\n",
                "    result = df.copy()\n",
                "    \n",
                "    # Amount deviation from mean\n",
                "    if 'amt' in result.columns or 'Amount' in result.columns:\n",
                "        amt_col = 'amt' if 'amt' in result.columns else 'Amount'\n",
                "        result['amount_deviation'] = (\n",
                "            result[amt_col] - result[amt_col].mean()\n",
                "        ) / (result[amt_col].std() + 1e-8)\n",
                "    \n",
                "    # Time anomaly (transactions outside 9am-9pm)\n",
                "    if 'trans_hour' in result.columns:\n",
                "        result['time_anomaly'] = (\n",
                "            (result['trans_hour'] < 6) | (result['trans_hour'] > 22)\n",
                "        ).astype(int)\n",
                "    \n",
                "    return result\n",
                "\n",
                "print('Feature engineering functions defined.')\n",
                "print('These would be applied to raw data before preprocessing.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "*Proceed to Notebook 04 for comprehensive model comparison and statistical analysis.*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}